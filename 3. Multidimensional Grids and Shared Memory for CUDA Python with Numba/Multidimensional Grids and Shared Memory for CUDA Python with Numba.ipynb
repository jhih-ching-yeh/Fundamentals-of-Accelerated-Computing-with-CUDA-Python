{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA Python與Numba中多維度Grid與Shared Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在此段落中，我們要再介紹一些在CUDA中使用Numba程式設計的技巧。首先說明CUDA如何提供建立2維和3維Block及Grid的能力，藉此促進處理2維和3維資料時的程式設計工作。接著要介紹由程式設計師管理的記憶體空間，稱為**共享記憶體(Shared Memory)**，可用於同一Block中Thread之間的快速溝通。您可利用這些技巧最佳化2維矩陣乘法Kernel。\n",
    "\n",
    "本段附錄中也透過範例說明如何減少矩陣轉置演算法的**共享記憶體 Bank conflicts**，並提供參考資訊連結。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目標\n",
    "\n",
    "完成本段必修部分後，您將能夠：\n",
    "\n",
    "* 利用多維度Block及Grid在多維度資料集上進行GPU加速平行作業。\n",
    "* 利用共享記憶體將資料暫存於晶片並減少全域記憶體存取。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2維及3維Block與Grid\n",
    "\n",
    "Grid和Block皆能配置成分別包含Block或Thread的2維或3維集合。對於經常使用2維或3維資料集的程式設計師來說，這主要是為了方便。以下是一個簡單的語法範例。您可能需要先閱讀Kernel定義及其啟動，才能明白這個概念。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = np.zeros(16).reshape(4, 4).astype(np.int32)\n",
    "d_A = cuda.to_device(A)\n",
    "\n",
    "@cuda.jit\n",
    "def add_2D_coordinates(A):\n",
    "    # By passing `2`, we get the thread's unique x and y coordinates in the 2D grid\n",
    "    x, y = cuda.grid(2)\n",
    "    \n",
    "    A[y][x] = x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2 3]\n",
      " [1 2 3 4]\n",
      " [2 3 4 5]\n",
      " [3 4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "# Here we create a 2D grid with 4 blocks in a 2x2 structure, each with 4 threads in a 2x2 structure\n",
    "# by using a Python tuple to signify grid and block dimensions.\n",
    "blocks = (2, 2)\n",
    "threads_per_block = (2, 2)\n",
    "\n",
    "add_2D_coordinates[blocks, threads_per_block](d_A)\n",
    "print(d_A.copy_to_host())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 關於練習的附加資訊"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "請切記，GPU必須有足夠支援SM的Grid大小，在處理作業並等待延遲終止的同時執行其他工作，才能夠展現良好性能。雖然如此，但本段中的幾項練習卻是要請您撰寫出不遵循這種最佳實務的Kernel。這是幫助您聚焦於語法和一些處理多重維度的基本模式，屬於介紹性質，較不具有實際效益。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習：在GPU上增設2D矩陣"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在此練習中，您要修改Kernel定義及其執行配置以平行執行2D矩陣加法。首先請撰寫一個只有在連同維度與資料匹配的Grid一起啟動時才能作用的Kernel。若有問題請參考[解決方案](../../../../edit/tasks/task3/task/solutions/add_matrix_solution.py)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function is written to add two 1D vectors. Refactor it to add 2D matrices\n",
    "\"\"\"\n",
    "@cuda.jit\n",
    "def add_matrix(A, B, C):\n",
    "    i = cuda.grid(1)\n",
    "    \n",
    "    C[i] = A[i] + B[i]\n",
    "\"\"\"\n",
    "\n",
    "@cuda.jit\n",
    "def add_matrix(A, B, C):\n",
    "    i,j = cuda.grid(2)\n",
    "    \n",
    "    C[j,i] = A[j,i] + B[j,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Do not modify the values in this cell, which defines 2D matrices of size 36*36\n",
    "A = np.arange(36*36).reshape(36, 36).astype(np.int32)\n",
    "B = A * 2\n",
    "C = np.zeros_like(A)\n",
    "d_A = cuda.to_device(A)\n",
    "d_B = cuda.to_device(B)\n",
    "d_C = cuda.to_device(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Refactor the launch configuration to use a 2D grid with 2D blocks\n",
    "blocks = 36\n",
    "threads_per_block = 36\n",
    "\n",
    "# This launch will throw a Typing error until refactor the definition above to operate on 2D arrays\n",
    "add_matrix[blocks, threads_per_block](d_A, d_B, d_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import testing\n",
    "output = d_C.copy_to_host()\n",
    "solution = A+B\n",
    "# This assertion will fail unles the output and solution are equal\n",
    "testing.assert_array_equal(output, solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習：在GPU 上實作2D矩陣乘法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在這項練習中，您要針對一個為2D[矩陣乘法](https://en.wikipedia.org/wiki/Matrix_multiplication)運算計算元素的邏輯製作其Kernel。就向您剛才撰寫的矩陣相加Kernel一樣，這也是個簡單的Kernel，需要與傳入矩陣的Grid維度相符。若有問題請參考[解決方案](../../../../edit/tasks/task3/task/solutions/matrix_multiply_solution.py)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda\n",
    "\"\"\"\n",
    "\n",
    "@cuda.jit\n",
    "def mm(a, b, c):\n",
    "    row, column = cuda.grid(2)\n",
    "    sum = 0\n",
    "    \n",
    "    ###\n",
    "    # TODO: Build the rest of this kernel to calculate the value for one element in the output matrix.\n",
    "    ###\n",
    "        \n",
    "    c[row][column] = sum\n",
    "\"\"\"    \n",
    "    \n",
    "from numba import cuda\n",
    "\n",
    "@cuda.jit\n",
    "def mm(a, b, c):\n",
    "    column, row = cuda.grid(2)\n",
    "    sum = 0\n",
    "    \n",
    "    for i in range(a.shape[0]):\n",
    "        sum += a[row][i] * b[i][column]\n",
    "        \n",
    "    c[row][column] = sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Do not modify the values in this cell\n",
    "a = np.arange(16).reshape(4,4).astype(np.int32)\n",
    "b = np.arange(16).reshape(4,4).astype(np.int32)\n",
    "c = np.zeros_like(a)\n",
    "\n",
    "d_a = cuda.to_device(a)\n",
    "d_b = cuda.to_device(b)\n",
    "d_c = cuda.to_device(c)\n",
    "\n",
    "grid = (2,2)\n",
    "block = (2,2)\n",
    "mm[grid, block](d_a, d_b, d_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import testing\n",
    "solution = a@b\n",
    "output = d_c.copy_to_host()\n",
    "# This assertion will fail until you successfully implement the kernel\n",
    "testing.assert_array_equal(output, solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多重維度的跨度迴圈"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正如同我們可利用Numba的`cuda.gridsize(1)`取得一個Grid中的Thread總數，我們也可以利用`cuda.gridsize(2)`得知2D Grid中每一方向Thread總數的變數。舉例而言，當2D資料集大於Grid，而我們希望每一個Thread都能跨越迴圈中的Grid而順利執行所有必要作業時，這種方法就極為實用。\n",
    "\n",
    "就像1D Grid跨步迴圈的情形一樣，這項技巧也能讓我們靈活設定Grid和Block的大小，不受資料形狀影響。以下範例說明2D Grid跨度迴圈的使用，最終列印出有關Grid中哪些Thread作用於矩陣中哪些元素的資訊。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda\n",
    "@cuda.jit\n",
    "def add_2D_coordinates_stride(A):\n",
    "\n",
    "    grid_y, grid_x = cuda.grid(2)\n",
    "    # By passing `2`, we get the grid size in both the x an y dimensions\n",
    "    stride_y, stride_x = cuda.gridsize(2)\n",
    "    \n",
    "    for data_i in range(grid_x, A.shape[0], stride_x):\n",
    "        for data_j in range(grid_y, A.shape[1], stride_y):\n",
    "            A[data_i][data_j] = grid_x + grid_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "現在我們建立一個在3x2 結構中具有6個Block的2D Grid，每個Block中也是由6個Thread構成的3x2結構。這個Grid小於我們的總資料集，且形狀無法平均配適資料集的維度。這個Kernel還是能夠存取資料中的每一元素。執行此單元後，嘗試隨著Grid形狀改動不同資料形狀。執行程式碼之前，先嘗試預測輸出矩陣的數值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2 3 4]\n",
      " [1 2 3 4 5]\n",
      " [2 3 4 5 6]\n",
      " [3 4 5 6 7]\n",
      " [0 1 2 3 4]\n",
      " [1 2 3 4 5]\n",
      " [2 3 4 5 6]\n",
      " [3 4 5 6 7]\n",
      " [0 1 2 3 4]\n",
      " [1 2 3 4 5]\n",
      " [2 3 4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "A = np.zeros(55).reshape(11, 5).astype(np.int32)\n",
    "d_A = cuda.to_device(A)\n",
    "\n",
    "blocks = (3, 2)\n",
    "threads_per_block = (3, 2)\n",
    "\n",
    "# With this configuration, `stride_x` will be 9, and `stride_y` will be 4\n",
    "add_2D_coordinates_stride[blocks, threads_per_block](d_A)\n",
    "print(d_A.copy_to_host())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習：處理大於Grid尺寸的2D矩陣加法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在此練習中，您要將簡單的矩陣加法Kernel修改為至少能夠處理任意大小資料集的尺寸。若有問題請參考[解決方案](../../../../edit/tasks/task3/task/solutions/add_matrix_stride_solution.py)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Currently this kernel will only work correctly when passed matrices that are of the same size as the grid.\n",
    "# Refactor using a strid in 2D so that it can work on data sets of an arbitrary size.\n",
    "\"\"\"\n",
    "@cuda.jit\n",
    "def add_matrix_stride(A, B, C):\n",
    "    j,i = cuda.grid(2)\n",
    "    \n",
    "    C[i,j] = A[i,j] + B[i,j]\n",
    "\"\"\"   \n",
    "    \n",
    "@cuda.jit\n",
    "def add_matrix_stride(A, B, C):\n",
    "\n",
    "    y, x = cuda.grid(2)\n",
    "    stride_y, stride_x = cuda.gridsize(2)\n",
    "    \n",
    "    for i in range(x, A.shape[0], stride_x):\n",
    "        for j in range(y, A.shape[1], stride_y):\n",
    "            C[i][j] = A[i][j] + B[i][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Please don't modify the values in this cell. They create a scenario where the data is\n",
    "# larger than the grid size.\n",
    "A = np.arange(64*64).reshape(64, 64).astype(np.int32)\n",
    "B = A * 2\n",
    "C = np.zeros_like(A)\n",
    "d_A = cuda.to_device(A)\n",
    "d_B = cuda.to_device(B)\n",
    "d_C = cuda.to_device(C)\n",
    "\n",
    "blocks = (6,6)\n",
    "threads_per_block = (6,6)\n",
    "\n",
    "add_matrix_stride[blocks, threads_per_block](d_A, d_B, d_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import testing\n",
    "output = d_C.copy_to_host()\n",
    "solution = A+B\n",
    "# This assertion will fail unles the output and solution are equal\n",
    "testing.assert_array_equal(output, solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習：處理大於Grid尺寸的2D矩陣乘法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在此練習中，您要完成一個能夠處理任意Grid和資料集形狀的矩陣乘法Kernel。您只需要處理包含`TODO`的兩行，利用`grid_`和`stride_`值來將執行Kernel的工作正確映射至資料中。若有問題請參考[解決方案](../../../../edit/tasks/task3/task/solutions/matrix_multiply_stride_solution.py)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda\n",
    "\"\"\"\n",
    "@cuda.jit\n",
    "def mm_stride(A, B, C):\n",
    "\n",
    "    grid_row, grid_column = cuda.grid(2)\n",
    "    stride_row, stride_column = cuda.gridsize(2)\n",
    "    \n",
    "    for data_row in range(0): # TODO: replace 0 with values that will correctly set data_row\n",
    "        for data_column in range(0): # TODO: replace 0 with values that will correctly set data_column\n",
    "            sum = 0\n",
    "            for i in range(A.shape[1]): # B.shape[0] would also be okay here\n",
    "                sum += A[data_row][i] * B[i][data_column]\n",
    "                \n",
    "            C[data_row][data_column] = sum\n",
    "\"\"\"\n",
    "\n",
    "@cuda.jit\n",
    "def mm_stride(A, B, C):\n",
    "\n",
    "    grid_column, grid_row = cuda.grid(2)\n",
    "    stride_column, stride_row = cuda.gridsize(2)\n",
    "    \n",
    "    for data_row in range(grid_row, A.shape[0], stride_row):\n",
    "        for data_column in range(grid_column, B.shape[1], stride_column):\n",
    "            sum = 0\n",
    "            for i in range(A.shape[1]): # `range(B.shape[0])` is also okay\n",
    "                sum += A[data_row][i] * B[i][data_column]\n",
    "                \n",
    "            C[data_row][data_column] = sum\n",
    "            \n",
    "n = 1024\n",
    "a = np.arange(n*n).reshape(n,n).astype(np.int32)\n",
    "b = np.arange(n*n).reshape(n,n).astype(np.int32)\n",
    "c = np.zeros((a.shape[0], b.shape[1])).astype(np.int32)\n",
    "\n",
    "d_a = cuda.to_device(a)\n",
    "d_b = cuda.to_device(b)\n",
    "d_c = cuda.to_device(c)\n",
    "\n",
    "ts = (32,32)\n",
    "bs = (32,32)\n",
    "\n",
    "mm_stride[bs, ts](d_a, d_b, d_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Please do not modify this cell. The strange dimensions of this data, and\n",
    "# the grid below are being set to make sure your kernel correctly handles arbitrary\n",
    "# data and grid sizes.\n",
    "\n",
    "a = np.arange(12).reshape(3,4).astype(np.int32)\n",
    "b = np.arange(24).reshape(4,6).astype(np.int32)\n",
    "c = np.zeros((a.shape[0], b.shape[1])).astype(np.int32)\n",
    "\n",
    "d_a = cuda.to_device(a)\n",
    "d_b = cuda.to_device(b)\n",
    "d_c = cuda.to_device(c)\n",
    "\n",
    "ts = (4, 3)\n",
    "bs = (3, 7)\n",
    "\n",
    "mm_stride[bs, ts](d_a, d_b, d_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import testing\n",
    "solution = a@b\n",
    "output = d_c.copy_to_host()\n",
    "# This assertion will fail until you correctly update the kernel above.\n",
    "testing.assert_array_equal(output, solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 共享記憶體\n",
    "\n",
    "至今我們已將主機與裝置記憶體區分開來，但只把裝置記憶體當成單一種類的記憶體。其實CUDA分為更細的[記憶體階層](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#memory-hierarchy)。我們至今所用過的裝置記憶體稱為**全域記憶體**，可為裝置上的任何Thread或Block所用，可隨著應用程式使用時間存在，記憶體空間相對較大。\n",
    "\n",
    "最後一個要討論的主題是如何利用On Chip memory上這個稱為**共享記憶體(Shared memory)**的區域。共享記憶體是一種由程式設計師定義的快取，其大小有限，[取決於所用GPU](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities)，且是由同一Block中的所有Thread**共用**。這是一項稀少的資源，無法由其所在Block以外的Thread所存取，且在Kernel完成執行後不會保留。但共享記憶體的頻寬遠大於全域記憶體，因此可於許多Kernel達成良好效用，特別是最佳化性能。\n",
    "\n",
    "以下是共享記憶體的常見用途：\n",
    "\n",
    " * 對讀取自全域記憶體且需要在Block內多次讀取的記憶體進行快取。\n",
    " * 緩衝來自Thread的輸出，使其可於寫回全域記憶體前先行合併。\n",
    " * 暫存Block內分scatter/gather操作的資料。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 共享記憶體語法\n",
    "\n",
    "Numba提供的[函數](https://numba.pydata.org/numba-doc/dev/cuda/memory.html#shared-memory-and-thread-synchronization)用於分配共享記憶體以及達成同一Block中Thread之間的同步，後者通常為對共享記憶體讀寫平行Thread之後所必須。\n",
    "\n",
    "宣告共享記憶體時，要提供共享陣列的形狀及其類型，使用[Numba類型](https://numba.pydata.org/numba-doc/dev/reference/types.html#numba-types)。**陣列的形狀必須為常數**，因此無法使用傳送至函數的引數，或是像`numba.cuda.blockDim.x`這種提供的變數，或是`cuda.griddim`的值。這個範例顯示的語法中以註解指出從主機記憶體到全域裝置記憶體，到共享記憶體，再回到全域裝置記憶體，最終回到主機記憶體的移動："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import types, cuda\n",
    "\n",
    "@cuda.jit\n",
    "def swap_with_shared(x, y):\n",
    "    # Allocate a 4 element vector containing int32 values in shared memory.\n",
    "    temp = cuda.shared.array(4, dtype=types.int32)\n",
    "    \n",
    "    idx = cuda.grid(1)\n",
    "    \n",
    "    # Move an element from global memory into shared memory\n",
    "    temp[idx] = x[idx]\n",
    "    \n",
    "    # cuda.syncthreads will force all threads in the block to synchronize here, which is necessary because...\n",
    "    cuda.syncthreads()\n",
    "    #...the following operation is reading an element written to shared memory by another thread.\n",
    "    \n",
    "    # Move an element from shared memory back into global memory\n",
    "    y[idx] = temp[cuda.blockDim.x - cuda.threadIdx.x - 1] # swap elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.arange(4).astype(np.int32)\n",
    "y = np.zeros_like(x)\n",
    "\n",
    "# Move host memory to device (global) memory\n",
    "d_x = cuda.to_device(x)\n",
    "d_y = cuda.to_device(y)\n",
    "\n",
    "swap_with_shared[1, 4](d_x, d_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 1, 0], dtype=int32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move device (global) memory back to the host\n",
    "d_y.copy_to_host()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 共享記憶體範例：矩陣轉置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此範例一方面說明共享記憶體的功用，另一方面再次說明2維CUDA程式設計，現在我們要撰寫的矩陣轉置Kernel是將以 row-major的2D陣列改為column-major 。(這是基於Mark Harris的[高效率矩陣轉置](https://devblogs.nvidia.com/parallelforall/efficient-matrix-transpose-cuda-cc/)部落格貼文)。\n",
    "\n",
    "首先進行簡單的方案，讓每一個Thread僅使用全域記憶體獨立讀寫個別元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TILE_DIM = 32\n",
    "BLOCK_ROWS = 8\n",
    "\n",
    "@cuda.jit\n",
    "def transpose(a_in, a_out):\n",
    "    y = cuda.blockIdx.x * TILE_DIM + cuda.threadIdx.x\n",
    "    x = cuda.blockIdx.y * TILE_DIM + cuda.threadIdx.y\n",
    "\n",
    "    for j in range(0, TILE_DIM, BLOCK_ROWS):\n",
    "        a_out[x + j, y] = a_in[y, x + j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[       0        1        2 ...     8189     8190     8191]\n",
      " [    8192     8193     8194 ...    16381    16382    16383]\n",
      " [   16384    16385    16386 ...    24573    24574    24575]\n",
      " ...\n",
      " [67084288 67084289 67084290 ... 67092477 67092478 67092479]\n",
      " [67092480 67092481 67092482 ... 67100669 67100670 67100671]\n",
      " [67100672 67100673 67100674 ... 67108861 67108862 67108863]]\n"
     ]
    }
   ],
   "source": [
    "size = 8192\n",
    "a_in = cuda.to_device(np.arange(size*size, dtype=np.int32).reshape((size, size)))\n",
    "a_out = cuda.device_array_like(a_in)\n",
    "\n",
    "print(a_in.copy_to_host())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.99 ms ± 4.11 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "[[       0     8192    16384 ... 67084288 67092480 67100672]\n",
      " [       1     8193    16385 ... 67084289 67092481 67100673]\n",
      " [       2     8194    16386 ... 67084290 67092482 67100674]\n",
      " ...\n",
      " [    8189    16381    24573 ... 67092477 67100669 67108861]\n",
      " [    8190    16382    24574 ... 67092478 67100670 67108862]\n",
      " [    8191    16383    24575 ... 67092479 67100671 67108863]]\n"
     ]
    }
   ],
   "source": [
    "grid_shape = (int(size/TILE_DIM), int(size/TILE_DIM))\n",
    "%timeit transpose[grid_shape,(TILE_DIM, BLOCK_ROWS)](a_in, a_out); cuda.synchronize()\n",
    "print(a_out.copy_to_host())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "現在我們使用共享記憶體將32x32區塊複製一次。 我們要用區塊大小的全域值，使其於編譯時為已知。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numba.types\n",
    "\n",
    "@cuda.jit\n",
    "def tile_transpose(a_in, a_out):\n",
    "    # THIS CODE ASSUMES IT IS RUNNING WITH A BLOCK DIMENSION OF (TILE_SIZE x TILE_SIZE)\n",
    "    # AND INPUT IS A MULTIPLE OF TILE_SIZE DIMENSIONS\n",
    "    tile = cuda.shared.array((TILE_DIM, TILE_DIM), numba.types.int32)\n",
    "\n",
    "    x = cuda.blockIdx.x * TILE_DIM + cuda.threadIdx.x\n",
    "    y = cuda.blockIdx.y * TILE_DIM + cuda.threadIdx.y\n",
    "    \n",
    "    for j in range(0, TILE_DIM, BLOCK_ROWS):\n",
    "        tile[cuda.threadIdx.y + j, cuda.threadIdx.x] = a_in[y + j, x] # move tile into shared memory\n",
    "\n",
    "    cuda.syncthreads()  # wait for all threads in the block to finish updating shared memory\n",
    "\n",
    "    # Compute transposed offsets\n",
    "    x = cuda.blockIdx.y * TILE_DIM + cuda.threadIdx.x\n",
    "    y = cuda.blockIdx.x * TILE_DIM + cuda.threadIdx.y\n",
    "\n",
    "    for j in range(0, TILE_DIM, BLOCK_ROWS):\n",
    "        a_out[y + j, x] = tile[cuda.threadIdx.x, cuda.threadIdx.y + j];\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.24 ms ± 38.4 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "[[       0     8192    16384 ... 67084288 67092480 67100672]\n",
      " [       1     8193    16385 ... 67084289 67092481 67100673]\n",
      " [       2     8194    16386 ... 67084290 67092482 67100674]\n",
      " ...\n",
      " [    8189    16381    24573 ... 67092477 67100669 67108861]\n",
      " [    8190    16382    24574 ... 67092478 67100670 67108862]\n",
      " [    8191    16383    24575 ... 67092479 67100671 67108863]]\n"
     ]
    }
   ],
   "source": [
    "a_out = cuda.device_array_like(a_in)\n",
    "\n",
    "%timeit tile_transpose[grid_shape,(TILE_DIM, BLOCK_ROWS)](a_in, a_out); cuda.synchronize()\n",
    "print(a_out.copy_to_host())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了已加速的代碼之外，這已經是一個非常好的加速效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 評估\n",
    "\n",
    "以下練習要請您充分運用至今所學。此處與先前練習的差異在於，沒有解決方案程式碼供您使用，並且您必須執行幾個額外的步驟以「執行評估」並取得您作答的分數。**開始處理前請仔細閱讀指示，以便順利完成評估。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 如何進行評估\n",
    "\n",
    "經執行以下步驟，完成評估：\n",
    "\n",
    "1. 遵循以下指示，按照您一般進行練習的方式處理下方單元。\n",
    "2. 達到您滿意的程度後，請遵循以下指示，複製程式碼並將之貼到連結的原始碼檔案中。貼入程式碼後請務必記得儲存檔案。\n",
    "3. 返回您用於啟動此本筆記的瀏覽器分頁，然後點擊**「評估(Assess)」**鈕。數秒後會顯示分數以及實用訊息。\n",
    "\n",
    "歡迎隨時點擊**評估**鈕，若您首次未能通過，您可額外修改程式碼並重複執行步驟1至3。祝您好運！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用共享記憶體進行矩陣乘法\n",
    "\n",
    "在此練習中，您所要完成的矩陣乘法Kernel可利用共享記憶體暫存來自輸入矩陣的數值，因此只需要被全域記憶體存取一次，之後Thread輸出元素的計算可利用快取值。此項評估旨在測試您能否推理2D平行問題並利用共享記憶體。這個問題的算術密度不大，並且我們不打算使用龐大資料集，因此相對於相當簡單的CPU版本，加速效果可能不大。但您將這些技巧運用於各種涉及2D資料集的程式加速需求中。\n",
    "\n",
    "為持續聚焦於共享記憶體，這個問題假設MxN和 NxM維度的輸入向量具有每一Block有NxN個Thread，且每一Grid有M/N個Block。這表示共享記憶體暫存與每一Block中Thread數量相等的元素，足以提供計算所需的所有輸入矩陣元素，且不需要Grid跨步。\n",
    "\n",
    "下圖顯示輸入矩陣、輸出矩陣、Block將計算其數值的輸出矩陣區域、Block將暫存的輸入矩陣區域以及該Block中單一Thread的輸出元素和輸入元素：\n",
    "\n",
    "![matrix multiply diagram](images/mm_image.png)\n",
    "\n",
    "共享記憶體塊取已經分配到Kernel中，您的任務有兩方面：\n",
    "1. 使用Block中的每一個Thread，在各快取中放入一個元素。\n",
    "2. 使用共享記憶體快取計算每一個Thread的`sum`值。\n",
    "\n",
    "務必執行可能需要的Thread同步，以免其他Thread寫入的尚未可用的快取值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Leave the values in this cell alone\n",
    "M = 128\n",
    "N = 32\n",
    "\n",
    "# Input vectors of MxN and NxM dimensions\n",
    "a = np.arange(M*N).reshape(M,N).astype(np.int32)\n",
    "b = np.arange(M*N).reshape(N,M).astype(np.int32)\n",
    "c = np.zeros((M, M)).astype(np.int32)\n",
    "\n",
    "d_a = cuda.to_device(a)\n",
    "d_b = cuda.to_device(b)\n",
    "d_c = cuda.to_device(c)\n",
    "\n",
    "# NxN threads per block, in 2 dimensions\n",
    "block_size = (N,N)\n",
    "# MxM/NxN blocks per grid, in 2 dimensions\n",
    "grid_size = (int(M/N),int(M/N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在修改以下單元的`mm_shared`之後，要執行評估前，請將此單元的內容貼入[**`assessment/definition.py`**](../../../../edit/tasks/task3/task/assessment/definition.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda, types\n",
    "\"\"\"\n",
    "@cuda.jit\n",
    "def mm_shared(a, b, c):\n",
    "    column, row = cuda.grid(2)\n",
    "    sum = 0\n",
    "\n",
    "    # `a_cache` and `b_cache` are already correctly defined\n",
    "    a_cache = cuda.shared.array(block_size, types.int32)\n",
    "    b_cache = cuda.shared.array(block_size, types.int32)\n",
    "\n",
    "    # TODO: use each thread to populate one element each a_cache and b_cache\n",
    "    \n",
    "    for i in range(a.shape[1]):\n",
    "        # TODO: calculate the `sum` value correctly using values from the cache \n",
    "        sum += a_cache[0][0] * b_cache[0][0]\n",
    "        \n",
    "    c[row][column] = sum\n",
    "\"\"\"\n",
    "    \n",
    "@cuda.jit\n",
    "def mm_shared(a, b, c):\n",
    "    sum = 0\n",
    "\n",
    "    # `a_cache` and `b_cache` are already correctly defined\n",
    "    a_cache = cuda.shared.array(block_size, types.int32)\n",
    "    b_cache = cuda.shared.array(block_size, types.int32)\n",
    "\n",
    "    # TODO: use each thread to populate one element each a_cache and b_cache\n",
    "    x,y = cuda.grid(2)\n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "    bpg = cuda.gridDim.x\n",
    "    TPB = int(N)\n",
    "    \n",
    "    for i in range(a.shape[1] / TPB):\n",
    "        a_cache[tx, ty] = a[x, ty + i * TPB]\n",
    "        b_cache[tx, ty] = b[tx + i * TPB, y]\n",
    "    \n",
    "    cuda.syncthreads()\n",
    "    for j in range(TPB):#a.shape[1]):\n",
    "        # TODO: calculate the `sum` value correctly using values from the cache \n",
    "        sum += a_cache[tx][j] * b_cache[j][ty]\n",
    "    cuda.syncthreads()    \n",
    "    c[x][y] = sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# There's no need to update this kernel launch\n",
    "mm_shared[grid_size, block_size](d_a, d_b, d_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not modify the contents in this cell\n",
    "from numpy import testing\n",
    "solution = a@b\n",
    "output = d_c.copy_to_host()\n",
    "# This assertion will fail until you correctly update the kernel above.\n",
    "testing.assert_array_equal(output, solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 總結\n",
    "\n",
    "完成本段後您能夠：\n",
    "\n",
    "* 利用多維度Block及Grid在多維度資料集上進行GPU加速平行作業。\n",
    "* 利用共享記憶體將資料暫存於晶片並減少較慢的全域記憶體存取。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下載內容\n",
    "\n",
    "如需下載本篇筆記內容，請執行以下單元，然後點擊下方的下載連結。請注意：若您是在本機Jupyter伺服器中執行此notebeook，notebook中的部分檔案路徑連結可能會因配合我們本身平台的資料夾結構而失效。您仍可透過Jupyter檔案瀏覽器查看檔案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./\r\n",
      "./.ipynb_checkpoints/\r\n",
      "./.ipynb_checkpoints/Multidimensional Grids and Shared Memory for CUDA Python with Numba-checkpoint.ipynb\r\n",
      "./solutions/\r\n",
      "./solutions/add_matrix_stride_solution.py\r\n",
      "./solutions/add_matrix_solution.py\r\n",
      "./solutions/matrix_multiply_solution.py\r\n",
      "./solutions/matrix_multiply_stride_solution.py\r\n",
      "./solutions/monte_carlo_pi_solution.py\r\n",
      "./images/\r\n",
      "./images/DLI Header.png\r\n",
      "./images/mm_image.png\r\n",
      "./assessment/\r\n",
      "./assessment/definition.py\r\n",
      "./Multidimensional Grids and Shared Memory for CUDA Python with Numba.ipynb\r\n",
      "tar: .: file changed as we read it\r\n"
     ]
    }
   ],
   "source": [
    "!tar -zcvf section3.tar.gz ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[下載本段檔案。](files/section3.tar.gz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 附錄：無Bank Conflict的矩陣轉置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "共享記憶體是儲存於**Banks**中。共有32個Banks可用於儲存共享記憶體，且不使用相同記憶體Bank的記憶體讀寫可以同時執行。當平行Thread試圖存取相同Bank中的記憶體時，即稱之為**Bank Conflict**，會導致操作序列化。即便發生Bank Conflict，共享記憶體的速度仍然很快，但建立可避免Bank Conflict的記憶體存取方式，就能進一步最佳化應用程式。\n",
    "\n",
    "我們在此僅使用一個範例，若需詳細資訊，可參考[高效率矩陣轉置](https://devblogs.nvidia.com/parallelforall/efficient-matrix-transpose-cuda-cc/)部落格貼文。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numba import cuda, types\n",
    "import numpy as np\n",
    "\n",
    "TILE_DIM = 32\n",
    "BLOCK_ROWS = 8\n",
    "TILE_DIM_PADDED = TILE_DIM + 1  # Read Mark Harris' blog post to find out why this improves performance!\n",
    "                                # https://devblogs.nvidia.com/parallelforall/efficient-matrix-transpose-cuda-cc/\n",
    "\n",
    "@cuda.jit\n",
    "def tile_transpose_no_bank_conflict(a_in, a_out):\n",
    "    # THIS CODE ASSUMES IT IS RUNNING WITH A BLOCK DIMENSION OF (TILE_SIZE x TILE_SIZE)\n",
    "    # AND INPUT IS A MULTIPLE OF TILE_SIZE DIMENSIONSx\n",
    "    tile = cuda.shared.array((TILE_DIM, TILE_DIM_PADDED), types.int32)\n",
    "\n",
    "    x = cuda.blockIdx.x * TILE_DIM + cuda.threadIdx.x\n",
    "    y = cuda.blockIdx.y * TILE_DIM + cuda.threadIdx.y\n",
    "    \n",
    "    for j in range(0, TILE_DIM, BLOCK_ROWS):\n",
    "        tile[cuda.threadIdx.y + j, cuda.threadIdx.x] = a_in[y + j, x] # move tile into shared memory\n",
    "\n",
    "    cuda.syncthreads()  # wait for all threads in the block to finish updating shared memory\n",
    "\n",
    "    # Compute transposed offsets\n",
    "    x = cuda.blockIdx.y * TILE_DIM + cuda.threadIdx.x\n",
    "    y = cuda.blockIdx.x * TILE_DIM + cuda.threadIdx.y\n",
    "\n",
    "    for j in range(0, TILE_DIM, BLOCK_ROWS):\n",
    "        a_out[y + j, x] = tile[cuda.threadIdx.x, cuda.threadIdx.y + j];\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[       0        1        2 ...     8189     8190     8191]\n",
      " [    8192     8193     8194 ...    16381    16382    16383]\n",
      " [   16384    16385    16386 ...    24573    24574    24575]\n",
      " ...\n",
      " [67084288 67084289 67084290 ... 67092477 67092478 67092479]\n",
      " [67092480 67092481 67092482 ... 67100669 67100670 67100671]\n",
      " [67100672 67100673 67100674 ... 67108861 67108862 67108863]]\n"
     ]
    }
   ],
   "source": [
    "size = 8192\n",
    "a_in = cuda.to_device(np.arange(size*size, dtype=np.int32).reshape((size, size)))\n",
    "a_out = cuda.device_array_like(a_in)\n",
    "\n",
    "print(a_in.copy_to_host())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.96 ms ± 2.99 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "[[       0     8192    16384 ... 67084288 67092480 67100672]\n",
      " [       1     8193    16385 ... 67084289 67092481 67100673]\n",
      " [       2     8194    16386 ... 67084290 67092482 67100674]\n",
      " ...\n",
      " [    8189    16381    24573 ... 67092477 67100669 67108861]\n",
      " [    8190    16382    24574 ... 67092478 67100670 67108862]\n",
      " [    8191    16383    24575 ... 67092479 67100671 67108863]]\n"
     ]
    }
   ],
   "source": [
    "grid_shape = (int(size/TILE_DIM), int(size/TILE_DIM))\n",
    "\n",
    "%timeit tile_transpose_no_bank_conflict[grid_shape,(TILE_DIM, BLOCK_ROWS)](a_in, a_out); cuda.synchronize()\n",
    "print(a_out.copy_to_host())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
